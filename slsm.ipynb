{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format(\"./toastynews.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = wv_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HKSL_src.csv')\n",
    "\n",
    "with open('wordpos-1689192352.csv', newline='', encoding=\"utf-8\") as posfile:\n",
    "      tmp = csv.reader(posfile, delimiter=',')\n",
    "      posrow = list(tmp)\n",
    "\n",
    "df.insert(6,'pos','')\n",
    "\n",
    "wordlist = df['Cantonese_Filtered'].tolist()\n",
    "\n",
    "words = []\n",
    "for i in range(len(posrow)):\n",
    "    words.append(posrow[i][0])\n",
    "    \n",
    "# Loop through the Cantonese_Filtered column\n",
    "for index, word in enumerate(wordlist):\n",
    "    if not word or (word not in words):\n",
    "        df.at[index, 'pos'] = 'NOTIN'\n",
    "    else:\n",
    "        # print(word)\n",
    "        pos = []\n",
    "        if words.count(word) > 1:\n",
    "            for i in range(len(words)):\n",
    "                if words[i] == word:\n",
    "                # if i == word and posrow[words.index(i)][1].strip('[\\'\\']') not in df.at[index, 'pos']:\n",
    "                    # df.at[index, 'pos'] += posrow[i][1].strip('[\\'\\']') + ','\n",
    "                    tmp = posrow[i][1].strip('[\\']')\n",
    "                    [pos.append(i.strip(' \\'')) for i in tmp.split(',') if i not in pos]\n",
    "                    # pos.append(tmp.split('\\', \\''))\n",
    "                    # pos = [*set(pos)]\n",
    "        else:\n",
    "            tmp = posrow[words.index(word)][1].strip('[\\'],')\n",
    "            # tmp.split('\\', \\'')\n",
    "            # tmp = [i.strip('\\'') for i in tmp]\n",
    "            # pos.append(tmp.split('\\', \\''))\n",
    "            # pos.append(tmp)\n",
    "            # print(tmp.split('\\', \\''))\n",
    "            # [print(i.strip(' \\'')) for i in tmp.split(',')]\n",
    "            [pos.append(i.strip(' \\'')) for i in tmp.split(',')]\n",
    "            \n",
    "        # tmp = df.at[index, 'pos'].strip('\\,')\n",
    "        # poslist = []\n",
    "        # for i in pos:\n",
    "        #     poslist.append(i)\n",
    "        # df.at[index, 'pos'] = poslist.sort()\n",
    "        # df.at[index, 'pos'] = pos.sort()\n",
    "        [i.strip('\\' ') for i in pos]\n",
    "        df.at[index, 'pos'] = sorted(pos)\n",
    "        # print(f\"{pos[0]} {type(pos[0])}  {pos[-1]} {type(pos[-1])}\")\n",
    "\n",
    "# Save the DataFrame to a new excel file\n",
    "df.to_excel('HKSL_pos.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我的 is not in vocab\n",
      "令人灰心 is not in vocab\n",
      "碌落去 is not in vocab\n",
      "遊船河 is not in vocab\n",
      "用電傳打字機 is not in vocab\n",
      "nan is not in vocab\n",
      "背包旅行 is not in vocab\n",
      "背包旅行 is not in vocab\n",
      "手語名 is not in vocab\n",
      "便衣警探 is not in vocab\n",
      "自然手語 is not in vocab\n",
      "交通津貼 is not in vocab\n",
      "膳食津貼 is not in vocab\n",
      "展能就業科 is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "臉色發白 is not in vocab\n",
      "眼凸凸 is not in vocab\n",
      "弱聽 is not in vocab\n",
      "nan is not in vocab\n",
      "玩滑板 is not in vocab\n",
      "聽音樂 is not in vocab\n",
      "玩電子遊戲 is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n",
      "nan is not in vocab\n"
     ]
    }
   ],
   "source": [
    "# Read excel file as a pandas DataFrame\n",
    "# df = pd.read_csv('HKSL_src.csv')\n",
    "df = pd.read_csv('HKSL_pos.csv')\n",
    "\n",
    "# Create a new column called \"corr_cantonese\"\n",
    "df.insert(7,'5th','')\n",
    "df.insert(8,'5th_pos','')\n",
    "df.insert(9,'5th_filename','')\n",
    "# # df.insert(7,'3rd','')\n",
    "# df.insert(8,'5th','')\n",
    "# df.insert(9,'10th','')\n",
    "# df.insert(10,'-1','')\n",
    "\n",
    "poslist = df['pos'].tolist()\n",
    "wordlist = df['cantonese_orig'].tolist()\n",
    "filenamelist = df['NewFileName'].tolist()\n",
    "\n",
    "curpos = []\n",
    "posindex = 0\n",
    "# Loop through the Cantonese_Filtered column\n",
    "for index, word in enumerate(wordlist):\n",
    "    if poslist[index] == 'NOTIN' or poslist[index] == 'EXCLUDED':\n",
    "        continue\n",
    "    if poslist[index] != curpos:\n",
    "        curpos = poslist[index]\n",
    "        posindex = index\n",
    "        \n",
    "    # Check if word is empty or not in vocab\n",
    "    if not word or (word not in wv.key_to_index):\n",
    "        df.at[index, '5th'] = 'NOTIN'\n",
    "        print(f\"{word} is not in vocab\")\n",
    "        # df.at[index, 'cantonese_map_furthest'] = 'NOTIN'\n",
    "    else:\n",
    "        # Find the most similar word in the wordlist\n",
    "        similarityList = []\n",
    "        similarityWordList = []\n",
    "        \n",
    "        for innerword in wordlist:\n",
    "            if innerword == word: \n",
    "                continue\n",
    "            if not innerword or (innerword not in wv.key_to_index): \n",
    "                continue\n",
    "            if poslist[wordlist.index(innerword)] != curpos:\n",
    "                continue\n",
    "            \n",
    "            similarity = wv.similarity(word, innerword)\n",
    "            \n",
    "            similarityList.append(similarity)\n",
    "            similarityList.sort(reverse=True)\n",
    "            idx = similarityList.index(similarity)\n",
    "            similarityWordList.insert(idx, innerword)   \n",
    "            \n",
    "            f = open(f'words/{word}.txt', \"w\", encoding=\"utf-8\")\n",
    "            # f.write(most_similar_word)\n",
    "            f.write(f\"{word} {poslist[index]}\")\n",
    "            f.write(word)\n",
    "            f.write(\"\\n\")\n",
    "            for i in range(len(similarityList)):\n",
    "                f.write(str(similarityWordList[i]))\n",
    "                f.write(\" \")\n",
    "                f.write(str(similarityList[i]))\n",
    "                f.write(\" \")\n",
    "                f.write(poslist[wordlist.index(innerword)])\n",
    "                if(i==0 or i==2 or i==4 or i==9 or i==len(similarityList)-1):\n",
    "                    f.write(f\" <-- {i+1}\")\n",
    "                f.write(\"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "                 \n",
    "        # Fill in the corr_cantonese column with the most similar word found\n",
    "        # df.at[index, '1st'] = similarityWordList[0]\n",
    "        # df.at[index, 'mapped_NewFileName'] = filenamelist[index]\n",
    "        # df.at[index, '3rd'] = similarityWordList[2]\n",
    "        if len(similarityWordList) > 4:\n",
    "            df.at[index, '5th'] = similarityWordList[4]\n",
    "            df.at[index, '5th_pos'] = poslist[wordlist.index(similarityWordList[4])]\n",
    "            df.at[index, '5th_filename'] = filenamelist[wordlist.index(similarityWordList[4])]\n",
    "        else:\n",
    "            df.at[index, '5th'] = len(similarityWordList)+1\n",
    "        # df.at[index, '5th'] = similarityWordList[4]\n",
    "        # df.at[index, '10th'] = similarityWordList[9]\n",
    "        # df.at[index, '-1'] = similarityWordList[-1]\n",
    "\n",
    "# Save the DataFrame to a new excel file\n",
    "df.to_excel('HKSL_added.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68507b45558adcbb6ed0c4260ae8a4ae5f03ef8f6c1246727ae98c95aa1ad230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
